{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_tHifYneW94"
   },
   "source": [
    "## <h1><center>Assignment 3: Contrastive Learning</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://www.cs.cornell.edu/courses/cs4782/2025sp/images/p3_header.jpeg\" style=\"width:45%;\">\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Goal:** In this project you will be exploring different strategies in contrastive pre-training. You will be using different computer vision models to generate meaningful image features that will later be used to perform image classification on cifar10, a widely used machine learning dataset.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**WHAT YOU'LL SUBMIT:** Your submission to Gradescope includes:\n",
    "\n",
    "\n",
    "1.   A `.zip` file uploaded to ***Coding Assignment 3*** [here](https://www.gradescope.com/courses/963234/assignments/5901455) containing the following files:\n",
    "\n",
    "<center>\n",
    "\n",
    "\\#|Files\n",
    "---|---\n",
    "i. | `submission.py`\n",
    "ii. |`linear_model_preds.csv`\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "2.   A `.pdf` version of `responses.tex` with responses to the questions in this notebook uploaded to ***Coding Assignment 3 Responses*** [here](https://www.gradescope.com/courses/963234/assignments/5901482).\n",
    "\n",
    "*More on how you are expected to access, modify and save these files as you follow along the instructions in the notebook.*\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**IMPORTANT:**\n",
    "\n",
    "This coding assignment requires training 3 separate models for (~30+ minutes each) and thus requires the use of GPUs. We have requested Google compute credits for every student in the class. These credits can be used to provision a GPU in Google Cloud, that can then be used in colab. To receive your compute credits and set up a Google Cloud instance, use the instructions [here](https://www.cs.cornell.edu/courses/cs4782/2025sp/docs/gc_guide.pdf).\n",
    "\n",
    "Things to keep in mind:\n",
    "*   **The GPU charges by the minute. Only use it when required and remember to delete the GPU job as soon as you're done.**  If you use up the credits given to you, we will not be able to provide you with more credits.\n",
    "*   You are only allowed one access code per person. The credits given using the code will be used across Coding Assignments 3, 4, 5 and the Final project. It is imperative that you use them wisely. In the case you run out of credits, we will not be able to award you more and you may not be able to complete future assignments!\n",
    "*   When selecting the GPU, select the region to be somewhere in the US.\n",
    "*   For GPU type, request either T4 or P100. In our experience, P100s are more easily available.\n",
    "*   If the GPU resource is not available, you can try to reserve a GPU in another region.\n",
    "*   To check which GPUs are assigned to you and to delete the GPU job, go [here](https://console.cloud.google.com/marketplace/product/colab-marketplace-image-public/colab) and click \"View Deployments\"\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**GOOGLE CLOUD:**\n",
    "\n",
    "While working on Google Cloud, there is no option to mount Google drive to access `submission.py` like you've done in previous projects. Instead, you **MUST** upload your `submission.py` to the session storage of your notebook on your Google Cloud instance. \n",
    "\n",
    "**WARNING:** Files in the session storage are automatically deleted when you close your Google Cloud instance, so you MUST download your `submission.py` if you make any changes to it on your Google Cloud instance before exiting. \n",
    "\n",
    "**Our recommendation:** Write your code in regular Colab as you have for the previous 2 assignments. Try to debug using the free T4 instance provided by Colab before launching your notebook on the Google Cloud instance for a final run through (you have ~2 hours of free T4 access a day which is not powerful enough to complete the entire assignment but can allow you to debug). Running on T4 GPU: You can click on the runtime option and change your runtime type to the T4 GPU (this should make your training faster).\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**DO's:**\n",
    "\n",
    "\n",
    "1.   **Recommendation:** Finish coding and debugging on regular Colab; only use the Google Cloud in the end to get the final results.\n",
    "2.   As before, all functionality you need to modify is within `submission.py`.\n",
    "3.   When on Google Cloud, upload your `submission.py` to the session storage.\n",
    "4.   Remember to execute all code cells sequentially, not just those youâ€™ve edited, to ensure your code runs properly.\n",
    "5.   Please cite any external sources you use to complete this assignment in your written responses.\n",
    "6.   Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**DONT's:**\n",
    "\n",
    "\n",
    "1.   DO NOT leave your GPU instance running after you are done working on this assignment. **AGAIN: The GPU charges by the minute. Only use it when required and remember to delete the GPU job as soon as you're done.**  If you use up the credits given to you, we will not be able to provide you with more credits.\n",
    "2.   DO NOT put your credit card on your Google Cloud account. If you accidentally leave your GPU instance running, you can be charged and will not be refunded! This has happened in the past!\n",
    "3.   DO NOT forget to download `submission.py` from the session storage in Google Cloud before closing your instance if you made any edits!\n",
    "2.   DO NOT change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with grading.\n",
    "3.   DO NOT delete any provided code/imports.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "***NOTE:***\n",
    "    \n",
    "*You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Ed discussion board to engage with your peers or seek assistance from the TAs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setting up the Colab environment.\n",
    "\n",
    "The first few code blocks will set up your Colab environment.  Upload the `a3_release` folder to your Google Drive and run/update the cells below, following the TODO instructions. Just like in the first assignment, you must specify the paths to your implementation so it can be accessed by this notebook (see *TODO 1*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BqtkCvuG-sw"
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install datasets\n",
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMqC9AKSeW95"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import transformers\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import umap\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "\n",
    "- If on **regular Google Colab**, run the following cell to mount your Drive!  \n",
    "- If on **Google Cloud**, SKIP this cell as mounting your Google Drive is not supported on Google Cloud. Instead, upload your `submission.py` to the session storage and run the cell after this one. \n",
    "- **DO NOT FORGET** to download your `submission.py` before closing your Google Cloud instance if you make any changes as files in the session storage are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 0: Mount your Google Drive; this allows the runtime environment to access your drive.\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# NOTE: Make sure your path does NOT include a '/' at the end!\n",
    "base_dir = \"/content/gdrive/MyDrive/<path-to-a3-release>\"\n",
    "sys.path.append(base_dir)\n",
    "## END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always run the following cell (even if on Google Cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes sure the submission module is reloaded whenever you make edits.\n",
    "%load_ext autoreload\n",
    "%aimport submission\n",
    "%autoreload 1\n",
    "import submission\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(F\"Device set to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GIaVWzHeW96"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSkjiIuKbd29"
   },
   "source": [
    "# Part 0.5: Setting up ResNet and Cifar10\n",
    "\n",
    "First, we provide a ResNet implementation, which will be trained to extract image features using the contrastive learning losses you will implement in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Er7C_X5XeW96"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channel, interm_channel, out_channel, stride=1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        in_channel = number of channels in the input to the first convolutional layer\n",
    "        interm_channel = number of channels in the output of the first convolutional layer\n",
    "                       = number of channels in the input to the second convolutional layer\n",
    "        out_channel = number of channels in the output\n",
    "        stride = stride for convolution, defaults to 1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, interm_channel, kernel_size = 3, stride = stride, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(interm_channel, out_channel, kernel_size = 3, stride = stride, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(in_channel, out_channel, kernel_size = 1, stride = stride) # 1x1 convolution\n",
    "        self.bn1 = nn.BatchNorm2d(interm_channel)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        x = self.conv3(x)\n",
    "        y +=  x # identity mapping\n",
    "        return F.relu(y)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_blocks, layer1_channel, layer2_channel, out_channel):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        num_blocks = number of blocks in a block layer\n",
    "        layer1_channel = number of channels in the input to the first block layer\n",
    "        layer2_channel = number of channels in the output of the first block layer\n",
    "                       = number of channels in the input to the second blcok layer\n",
    "        out_channel = number of channels in the output\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.first = nn.Sequential(\n",
    "            nn.Conv2d(3, layer1_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(layer1_channel), nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.last = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.layer1 = self.block_layer(num_blocks, layer1_channel, layer2_channel)\n",
    "        self.layer2 = self.block_layer(num_blocks, layer2_channel, out_channel)\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(out_channel, out_channel),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_channel, out_channel)\n",
    "        )\n",
    "\n",
    "\n",
    "    def block_layer(self, num_blocks, in_channel, out_channel):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        num_blocks = number of blocks in the block layer\n",
    "        in_channel = number of input channels to the entire block layer\n",
    "        out_channel = number of output channels in the output of the entire block layer\n",
    "        \"\"\"\n",
    "        blk = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0:\n",
    "                blk.append(ResidualBlock(in_channel, out_channel, out_channel))\n",
    "            else:\n",
    "                blk.append(ResidualBlock(out_channel, out_channel, out_channel))\n",
    "\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        # x: (batch_size, 3, 32, 32)\n",
    "        y = self.first(x)\n",
    "        y = self.layer1(y)\n",
    "        # 2x2 avg pooling\n",
    "        y = F.avg_pool2d(y, kernel_size=2, stride=2)\n",
    "        y = self.layer2(y)\n",
    "        y = self.last(y)\n",
    "        if return_embedding:\n",
    "            return y\n",
    "        y = self.projection_head(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXwC_ZuWb-E1"
   },
   "source": [
    "The following code block defines the dataset we will be using for this assignment. We will be using the [Cifar10](https://huggingface.co/datasets/cifar10) dataset, which contains color images of size 32x32 for 10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck).\n",
    "\n",
    "We will be creating positive pairs of images by performing two different, randomized image transformations on the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McZQBf4peW96"
   },
   "outputs": [],
   "source": [
    "# Load cifar10 data from Hugging Face\n",
    "cifar10 = load_dataset('cifar10')\n",
    "\n",
    "# Load the data\n",
    "train_data = cifar10['train']\n",
    "test_data = cifar10['test']\n",
    "\n",
    "# Define randomized transforms for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(2, 9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define fixed SimCLR transforms for test-time\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the SimCLR dataset\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, data, transform, split='train'):\n",
    "        self.split = split\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]['img']\n",
    "        if self.split == 'train':\n",
    "            image1 = self.transform(image)\n",
    "            image2 = self.transform(image)\n",
    "            return image1, image2\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "            return image\n",
    "\n",
    "# Create the SimCLR dataset\n",
    "train_dataset = SimCLRDataset(train_data, train_transform, split='train')\n",
    "val_dataset = SimCLRDataset(train_data, test_transform, split='test')\n",
    "test_dataset = SimCLRDataset(test_data, test_transform, split='test')\n",
    "\n",
    "# Create the SimCLR dataloaders\n",
    "BATCH_SIZE = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCRklGuXjwKe"
   },
   "source": [
    "## Part 1: Contrastive Learning Loss Functions (55 pts)\n",
    "\n",
    "For this assignment, you will be implementing two different contrastive learning loss functions, the triplet loss and the SimCLR loss.\n",
    "\n",
    "### Part 1.1: Triplet Loss (25 pts)\n",
    "\n",
    "The formula for the triplet loss is as follows:\n",
    "$$Loss = \\texttt{max}(0,\\text{sim}(x_i,\\ x_i^n) - \\text{sim}(x_i,\\ x_i^p) + m)$$\n",
    "\n",
    "where $x_i$ is a training example, $x_i^n$ and $x_i^p$ are negative and positive examples of $x_i$, respectively, $m$ is the margin, and $\\text{sim}(a,b)$ is some similarity function.\n",
    "\n",
    "You will implement the `triplet_loss` function as follows:\n",
    "\n",
    "1.   L2-normalize of each of the input training examples.\n",
    "2.   Calculate the triplet loss between all possible triplets of negative and positive examples.\n",
    "\n",
    "        a. The similarity function is the dot product of the examples, i.e. $\\text{sim}(a, b) = a(b)^T$\n",
    "\n",
    "        b. `queries` and `keys` both contain $b$ examples, where $b$ is the batch size. The $i^{th}$ example in `keys` is a **positive example** for the $j^{th}$ example in `queries` if $i = j$. Otherwise, if $i \\neq j$, then the $i^{th}$ example in `keys` is a **negative example** for the $j^{th}$ example in `queries`.\n",
    "\n",
    "3. Average the loss across all triplets.\n",
    "\n",
    "The following PyTorch functions may be helpful for implementing the triplet loss:\n",
    "\n",
    "\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.eye.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4V8Syh4eW96"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the triplet_loss function in submission.py\n",
    "from submission import triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCoi9er9j4VZ"
   },
   "source": [
    "Now that we have implemented the triplet loss, we can train our ResNet to produce meaningful image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AQYlyLPeW97"
   },
   "outputs": [],
   "source": [
    "# Define the triplet model\n",
    "triplet_model = ResNet(2, 64, 128, 256).cuda()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(triplet_model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=NUM_EPOCHS * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VekzusTeW97"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for image1, image2 in tqdm(loader):\n",
    "        image1 = image1.to(device)\n",
    "        image2 = image2.to(device)\n",
    "\n",
    "        queries = model(image1)\n",
    "        keys = model(image2)\n",
    "\n",
    "        loss = loss_fn(queries, keys)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will train the ResNet using the triplet loss you just implemented. This cell may take several minutes to run (~30-40 minutes on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VR_IBKKmeW97",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "loss_fn = triplet_loss\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(triplet_model, train_dataloader, optimizer, scheduler, loss_fn)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "# Test the model by extracting features and training a linear classifier\n",
    "triplet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJA_1OYnveBM"
   },
   "source": [
    "Before we can use the image features produced by the ResNet to help our image classification model, we need to define a function, `extract_features`, that will pass each of our images through the pre-trained model to extract the corresponding image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVtVGeUKeW98"
   },
   "outputs": [],
   "source": [
    "def extract_features(model, val_dataloader):\n",
    "    features = []\n",
    "    pixels = []\n",
    "    with torch.no_grad():\n",
    "        for image in val_dataloader:\n",
    "            image = image.cuda()\n",
    "            pixels.append(image.to('cpu'))\n",
    "            feature = model(image, return_embedding=True)\n",
    "            features.append(feature)\n",
    "    features = torch.cat(features).to('cpu').numpy()\n",
    "    return features, pixels\n",
    "\n",
    "features, train_img_features = extract_features(triplet_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADovCtF2fDrd"
   },
   "source": [
    "The following code will train a linear classifier on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ekp5si3eW98",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subsample = np.random.choice(features.shape[0], size=5000, replace=False)\n",
    "features_subsample = features[subsample]\n",
    "train_data_label = np.array(train_data['label'])[subsample]\n",
    "\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression(max_iter=100, solver='saga', multi_class='multinomial', verbose=1))\n",
    "classifier.fit(features_subsample, train_data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPzbNOxwfXU5"
   },
   "source": [
    "To examine how beneficial these image features are for classification, we can compare the model trained using the ResNet image features to a linear model that simply uses the pixels of the original image as an input. The following code will train the pixel-space classifier. This cell may take 1-2 minutes to run on GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxASvY1avFiN"
   },
   "outputs": [],
   "source": [
    "# Train the pixel-space classifier\n",
    "train_img_array = np.concatenate(train_img_features, axis=0)\n",
    "\n",
    "train_img_array = train_img_array[subsample]\n",
    "pixel_classifier = make_pipeline(StandardScaler(), LogisticRegression(max_iter=100, solver='saga', multi_class='multinomial', verbose=1))\n",
    "pixel_classifier.fit(train_img_array.reshape(-1, 32 * 32 * 3), train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSC19rrieW98"
   },
   "outputs": [],
   "source": [
    "test_features, test_img_features = extract_features(triplet_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_8Cg38mFaI6"
   },
   "source": [
    "Finally, let us compare the performance of our two models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftqtDvnoeW98"
   },
   "outputs": [],
   "source": [
    "# Test the feature classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "triplet_accuracy = accuracy_score(test_data['label'], predictions)\n",
    "print(f'Accuracy: {triplet_accuracy:.4f}')\n",
    "\n",
    "# Convert the 'img' list into a numpy array\n",
    "test_img_array = np.concatenate(test_img_features, axis=0)\n",
    "\n",
    "# Test the pixel-space classifier\n",
    "pixel_predictions = pixel_classifier.predict(test_img_array.reshape(-1, 32 * 32 * 3))\n",
    "pixel_accuracy = accuracy_score(test_data['label'], pixel_predictions)\n",
    "print(f'Pixel Accuracy: {pixel_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR1qSMleFa5-"
   },
   "source": [
    "To reference the performance of our models later, you can run this cell to save the model accuracies to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "revaTeuHntwU"
   },
   "outputs": [],
   "source": [
    "# Save accuracy to a file\n",
    "with open(f'accuracy_epoch{NUM_EPOCHS}_0.json', 'w') as f:\n",
    "    json.dump({'triplet_accuracy': triplet_accuracy, 'pixel_accuracy': pixel_accuracy}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cviW2JblF0ao"
   },
   "source": [
    "### Q1: How do the performances of the two models you just trained compare? What do you think might contribute to the differences you noticed? Write 2-3 sentences. (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z04awns-F0kb"
   },
   "source": [
    "**Answer: add your answer to responses.tex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYgXj1YDnrqU"
   },
   "source": [
    "### Part 1.2: SimCLR Loss (25 pts)\n",
    "\n",
    "Another popular contrastive learning framework is SimCLR, which learns representations by maximizing the agreement between differently augmented views of the same training example. Given $2B$ training examples, the loss for SimCLR is calculated via the following steps:\n",
    "\n",
    "\n",
    "1.   L2-normalize of each of the input training examples.\n",
    "2.   Calculate the similarity matrix $S$, where $S_{ij} = x_i(x_j)^T \\ \\ \\forall i,j \\in \\{1,...,2B\\}$.\n",
    "3.   Let the loss between two examples $l(i,j) = -\\log \\left(\\frac{\\exp(S_{ij}/ \\tau)}{\\sum_{k=1}^{2B}\\mathbf{1}_{k\\neq i}\\exp(S_{ik}/ \\tau)}\\right)$.\n",
    "     \n",
    "     Calculate the loss $L = \\frac{1}{2B} \\sum_{k=1}^B l(k, 2k) + l(2k, k)$, where the $k^{th}$ and $2k^{th}$ images are two differently-augmented views of the same image.\n",
    "\n",
    "\n",
    "Here are some PyTorch functions that maybe be useful in implementing this loss:\n",
    "\n",
    "\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.eye.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVThTpeoeW98"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the nt_xent_loss function in submission.py\n",
    "from submission import nt_xent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddYMnOGxFcov"
   },
   "source": [
    "Now that we have implemented the SimCLR loss, we can, again, train a ResNet with it to produce image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIfbAI3oeW98"
   },
   "outputs": [],
   "source": [
    "# Define the SimCLR model\n",
    "simclr_model = ResNet(2, 64, 128, 256)\n",
    "simclr_model = simclr_model.cuda()\n",
    "\n",
    "simclr_model = simclr_model\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(simclr_model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=NUM_EPOCHS * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will train the ResNet using the SimCLR loss you just implemented. This cell may take several minutes to run (~30-40 minutes on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iF3GkrEweW98"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "loss_fn = nt_xent_loss\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(simclr_model, train_dataloader, optimizer, scheduler, loss_fn)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-G97mEAjqRk"
   },
   "source": [
    "The following code will train another logistic regression model to perform image classification, using the SimCLR image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho9MF0K-eW98"
   },
   "outputs": [],
   "source": [
    "# Test the model by extracting features and training a linear classifier\n",
    "simclr_model.eval()\n",
    "\n",
    "features, train_img_features = extract_features(simclr_model, val_dataloader)\n",
    "test_features, test_img_features = extract_features(simclr_model, test_dataloader)\n",
    "\n",
    "features_subsample = features[subsample]\n",
    "train_data_label = np.array(train_data['label'])[subsample]\n",
    "\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression(max_iter=100, solver='saga', multi_class='multinomial', verbose=1))\n",
    "classifier.fit(features_subsample, train_data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAPQA5iqj4ax"
   },
   "source": [
    "Finally, let us compare how the classifier using SimCLR features compares to the previous classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEd9gpe8eW98"
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_features)\n",
    "simclr_accuracy = accuracy_score(test_data['label'], predictions)\n",
    "print(f'Accuracy: {simclr_accuracy:.4f}')\n",
    "\n",
    "with open(f'accuracy_epoch{NUM_EPOCHS}_1.json', 'w') as f:\n",
    "    json.dump({'simclr_accuracy': simclr_accuracy,'triplet_accuracy': triplet_accuracy, 'pixel_accuracy': pixel_accuracy}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EOfBy5FeW98"
   },
   "source": [
    "## Part 2: Vision Transformers (ViTs) (45 pts)\n",
    "\n",
    "Another popular model for image tasks is the Vision Transformer (ViT), a transformer adapted for processing images instead of sequences. In this portion of the assignment, you will be implementing a ViT model that we will later train to produce image features to be used for image classification, similar to the ResNet we used in part 1.\n",
    "\n",
    "ViTs contain many of the same components as the Transformer you implemented for the previous project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcfUgNRbrlPF"
   },
   "source": [
    "We provide the function, `posemb_sincod_2d`, to produce the positional encoding for image patches. Since images are 2-dimensional (rather than 1-dimensional like text), the function is slightly different from the one we used in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nb-V-6DfkUDd"
   },
   "outputs": [],
   "source": [
    "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
    "    '''\n",
    "    h: Height of the patch.\n",
    "    w: Width of the patch.\n",
    "    dim: The dimension of the model embeddings.\n",
    "    '''\n",
    "\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
    "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "\n",
    "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
    "    omega = 1.0 / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INdDgacLkStj"
   },
   "source": [
    "### Part 2.1: ViT Implementation (30 pts)\n",
    "You will be implementing a ViT as depicted in the diagram below.\n",
    "\n",
    "<center><img src=\"https://www.cs.cornell.edu/courses/cs4782/2025sp/images/vit.png\"/></center>\n",
    "\n",
    "\n",
    "The architecture you will implement is as follows:\n",
    "\n",
    "1.   `to_patch_embedding`, which will:\n",
    "      1.  Rearrange the training images (`b` x `c` x `h` x `w`) into flattened patches (`b` x `number of patches` x `size of patches`).\n",
    "      2.  Pass the flattened patches through a LayerNorm layer.\n",
    "      3.  Project the LayerNorm output up to the dimension of the Transformer Encoder, `d_model`.\n",
    "      4.  Pass the projected embeddings through a second LayerNorm layer.\n",
    "2.   `pos_embedding`, which will add the 2D positional embeddings produced by `posemb_sincod_2d` to the outputs of the second LayerNorm Layer.\n",
    "3.   `encoder`, instead of implementing the transformer encoder from scratch, use [nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html). The encoder has `num_layer` layers, `d_model` features, `num_heads` attention heads, `d_ff` feedforward layer dimensionality, and a dropout probability of `p`. Note that there is a dropout layer (with dropout probability of `p`) applied before the encoder and a LayerNorm applied after the encoder. While there are no naming restrictions for the dropout layer, ensure to call the LayerNorm layer `output_ln`.\n",
    "3.   `projection_head`, which consists of a MLP with 2 layers similar to the ResNet model. There is a SiLU activation after the first layer. Each MLP layer takes as input `d_model` features and produces `d_model` features. `projection_head` should only be used if `return_embedding` is False.\n",
    "\n",
    "\n",
    "Here are some functions that might be helpful in the implementation of the ViT:\n",
    "\n",
    "*   https://einops.rocks/api/rearrange/\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNPtk9zmeW99"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the ViT class in submission.py\n",
    "from submission import ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPnCcz6OkddR"
   },
   "source": [
    "## ViT Training\n",
    "\n",
    "The following cells will train the ViT model using the SimCLR loss function implemented in part 1. This cell may take several minutes to run (~30-40 minutes on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BnMnOxseW99"
   },
   "outputs": [],
   "source": [
    "simclr_vit = ViT(256, 4)\n",
    "simclr_vit = simclr_vit.cuda()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(simclr_vit.parameters(), lr=0.001)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=NUM_EPOCHS * len(train_dataloader))\n",
    "\n",
    "# Train the model\n",
    "loss_fn = nt_xent_loss\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(simclr_vit, train_dataloader, optimizer, scheduler, loss_fn)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arhpL4F-44kE"
   },
   "source": [
    "Next, we can train another linear classifier using the SimCLR features produced by the ViT we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "640VtQNeeW99"
   },
   "outputs": [],
   "source": [
    "# Test the model by extracting features and training a linear classifier\n",
    "simclr_vit.eval()\n",
    "\n",
    "features, _ = extract_features(simclr_vit, val_dataloader)\n",
    "test_features, _ = extract_features(simclr_vit, test_dataloader)\n",
    "\n",
    "subsample = np.random.choice(features.shape[0], size=5000, replace=False)\n",
    "\n",
    "features_subsample = features[subsample]\n",
    "train_data_label = np.array(train_data['label'])[subsample]\n",
    "\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression(max_iter=100, solver='saga', multi_class='multinomial', verbose=1))\n",
    "classifier.fit(features_subsample, train_data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiUgWdGK5Riv"
   },
   "source": [
    "Finally, let us compare how the ViT performed compared to the previous models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UETDvqCteW99"
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_features)\n",
    "simclr_vit_accuracy = accuracy_score(test_data['label'], predictions)\n",
    "print(f'Accuracy: {simclr_vit_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ljIb-tJljWu"
   },
   "source": [
    "If you would like to save the test accuracies of the four classification models for future reference, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhajrQTGeW99"
   },
   "outputs": [],
   "source": [
    "with open(f'accuracy_epoch{NUM_EPOCHS}_2.json', 'w') as f:\n",
    "    json.dump({'simclr_vit_accuracy': simclr_vit_accuracy, 'simclr_accuracy': simclr_accuracy,'triplet_accuracy': triplet_accuracy, 'pixel_accuracy': pixel_accuracy}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZUEQwvfy5RY"
   },
   "source": [
    "## Part 2.2: Image Feature Visualization\n",
    "Similar to how we visualized the word embeddings in Homework 2, we can also visualize the image features obtained from our ViT model using UMAP (similar to tSNE).\n",
    "\n",
    "The following code will produce two plots. Each point represents a test set image and is colored according to its label. The first plot visualizes the image features obtained from the ViT. The second plot visualizes the same points (images) using the image pixels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtwaoLm2CyvG"
   },
   "outputs": [],
   "source": [
    "# Visualize the features\n",
    "class2name_list = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Reduce the dimensionality of the features\n",
    "reducer = umap.UMAP()\n",
    "scaled_test_features = StandardScaler().fit_transform(test_features)\n",
    "\n",
    "reduced_features = reducer.fit_transform(scaled_test_features)\n",
    "\n",
    "# Plot the features\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    test_label_array = np.array(test_data['label'])\n",
    "    mask = test_label_array == i\n",
    "    subsample = np.random.choice(np.where(mask)[0], size=100, replace=False)\n",
    "    plt.scatter(reduced_features[subsample, 0], reduced_features[subsample, 1], label=class2name_list[i])\n",
    "plt.title('Contrastive Feature Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f'features_epoch{NUM_EPOCHS}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2Ix7_C2eW99"
   },
   "outputs": [],
   "source": [
    "# Visualize the pixel space\n",
    "# Reduce the dimensionality of the pixel space\n",
    "reducer = umap.UMAP()\n",
    "test_img_array = np.concatenate(test_img_features, axis=0)\n",
    "reduced_pixels = reducer.fit_transform(test_img_array.reshape(-1, 32 * 32 * 3))\n",
    "\n",
    "# Plot the pixel space\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    mask = test_label_array == i\n",
    "    subsample = np.random.choice(np.where(mask)[0], size=100, replace=False)\n",
    "    plt.scatter(reduced_pixels[subsample, 0], reduced_pixels[subsample, 1], label=class2name_list[i])\n",
    "plt.title('Pixel-Space Feature Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f'pixels_epoch{NUM_EPOCHS}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b24_SLGey6XI"
   },
   "source": [
    "### Q2: How do the visualizations of the contrastive learning features compare to the visualization of the image pixels? How tightly clustered are the points for the different classes in each of the two visualizations? How might your observations relate to the utility of these two sets of features for image classification? Write 3-4 sentences. (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: add your answer to responses.tex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the following to create your submission files.\n",
    "`linear_model_preds.csv` tests the accuracy of your ViT implementation trained with the SimCLR loss. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_vit.eval()\n",
    "\n",
    "features, train_img_features = extract_features(simclr_model, val_dataloader)\n",
    "test_features, test_img_features = extract_features(simclr_model, test_dataloader)\n",
    "\n",
    "subsample = np.random.choice(features.shape[0], size=5000, replace=False)\n",
    "\n",
    "features_subsample = features[subsample]\n",
    "train_data_label = np.array(train_data['label'])[subsample]\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        max_iter=100, solver=\"saga\", multi_class=\"multinomial\", verbose=1\n",
    "    ),\n",
    ")\n",
    "classifier.fit(features_subsample, train_data_label)\n",
    "\n",
    "predictions = classifier.predict(test_features)\n",
    "pred_df = pd.DataFrame(predictions, columns=[\"preds\"])\n",
    "pred_df.to_csv(\"linear_model_preds.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'linear_model_preds.csv'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
