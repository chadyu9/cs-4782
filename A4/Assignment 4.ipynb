{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IiN8RKHRDlc"
   },
   "source": [
    "# GPU Usage:\n",
    "\n",
    "**Recommendation:** Finish coding and debugging on CPU; only use the GPU in the end to get the final results.\n",
    "\n",
    "**Important.** Things to keep in mind:\n",
    "\n",
    "- **The GPU charges by the minute. Only use it when required and remember to delete the GPU job as soon as you're done.** If you use up the credits given to you, we will not be able to provide you with more credits.\n",
    "- To check which GPUs are assigned to you and to delete the GPU job, go [here](https://console.cloud.google.com/marketplace/product/colab-marketplace-image-public/colab) and click \"View Deployments\". **Disconnecting the GPU from Colab does not delete the instance.** You need to delete it at the provided link.\n",
    "- When selecting the GPU, select the region to be somewhere in the US.\n",
    "- For GPU type, request either T4 or P100. In our experience, P100s are more easily available.\n",
    "- If the GPU resource is not available, you can try to reserve a GPU in another region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baGIh3V9W-Tf"
   },
   "source": [
    "## Project 4: Generative Modeling\n",
    "\n",
    "In this project, we will be exploring different approaches for generative modeling. In the first part, we will implement a VAE to draw samples of handwritten digits. In the second part, we will learn a diffusion model to sample from a synthetic 2-D dataset.\n",
    "\n",
    "**WHAT YOU'LL SUBMIT**: Your submission to Gradescope includes:\n",
    "1. Your completed `submission.py` file uploaded to ***Coding Assignment 4***.\n",
    "2. A `.pdf` file with responses to questions in the notebook and your **generated images** uploaded to ***Coding Assignment 4 Responses***.\n",
    "\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code in the <em> #TODO </em> and <em> ####### </em> sections within the cells of this notebook. Only cells in which you write code will be graded. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with grading. Also, remember to execute all code cells sequentially, not just those youâ€™ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "Please install all of the necessary packages shown in the code box below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaB4q0IpGOAy"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install einops\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etVelp5aW6UA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from functools import partial\n",
    "import transformers\n",
    "import matplotlib.animation\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "import sys\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Mount your Google Drive; this allows the runtime environment to access your drive.\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# NOTE: Make sure your path does NOT include a '/' at the end!\n",
    "base_dir = \"/content/gdrive/MyDrive/\"\n",
    "sys.path.append(base_dir)\n",
    "## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes sure the submission module is reloaded whenever you make edits.\n",
    "%load_ext autoreload\n",
    "%aimport submission\n",
    "%autoreload 1\n",
    "import submission\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(F\"Device set to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLDn4a_N3YZ7"
   },
   "source": [
    "# Part 1: Variational Autoencoder (VAEs) (50 pts)\n",
    "\n",
    "In this part, you will be training a varianational autoencoder on the MNIST dataset, which consists of a large collection of 28x28 pixel grayscale images of handwritten digits (0-9). Upon successful training, the VAE will be capable of generating new digit images that look like those in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qaw0-MadFkFu"
   },
   "outputs": [],
   "source": [
    "# set up the dataset and dataloaders\n",
    "\n",
    "# cuatom dataset\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]['image']\n",
    "        image = self.transform(image)\n",
    "        label = self.data[idx]['label']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = load_dataset('mnist')\n",
    "train_dataset = MNISTDataset(dataset['train'], transform)\n",
    "test_dataset = MNISTDataset(dataset['test'], transform)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlzoyU--WldZ"
   },
   "source": [
    "### Part 1.1: VAE Implementation (20 pts)\n",
    "\n",
    "You will be implementing a VAE as depicted in the diagram below.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1JLvO2yVDUOsb-MHQdXDuPrUx0L9huQ7f\" width=\"650\" height=\"400\"/></center>\n",
    "\n",
    "Here are more details:\n",
    "\n",
    "1. The input to the VAE is 28 x 28 dimensional MNIST images.\n",
    "2. The `__init__()` method defines the following layers:\n",
    "3. Two fully connected encoder layers- the first one produces `h_dim1=512` features and second one produces `h_dim2=256` features (you should be able to figure out the number of input features).\n",
    "4. A fully connected layer that maps the `h_dim2=256` features to `z_dim=32` features that representa mean.\n",
    "5. Another fully connected layer that map the `h_dim2=256` features to `z_dim=32` features that representa the log variance.\n",
    "6. Two fully connected decoder layers- the first one produces `h_dim1=256` features and second one produces `h_dim2=512` features (you should be able to figure out the number of input features).\n",
    "7. An output fully connected layer that produces `x_dim=784` features.\n",
    "8. `encoder()` defines the encoder forward pass and returns the mean ${\\boldsymbol{\\mathbf{\\mu}}}$ and log variance $\\log {\\boldsymbol{\\mathbf{\\sigma}}}^2$ for ever sample in the batch.\n",
    "9. `decoder()` defines the decoder forward pass and should return the output which has `x_dim=784` features. **Note: Add a sigmoid activation before you return the output to ensure that the values produced are between 0 and 1.**\n",
    "10. `sample()` returns the sampled latent vectors. Dimension $i$ of the vector should be sampled from $\\mathcal{N}(\\mu_i, \\sigma_i)$. You we will need to use the re-parametrization trick as discussed in class. More details are provided below in `sample()`.\n",
    "11. `forward()` calls `encoder()`, `sample()`, and `decoder()` to return the flattended reconstruction, the mean and log_variance vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement VAE in `submission.py`** then run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHDnt6pGKSwG"
   },
   "outputs": [],
   "source": [
    "from submission import VAE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=32)\n",
    "if torch.cuda.is_available():\n",
    "    vae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br9pCsQp7rjJ"
   },
   "source": [
    "### Part 1.2: VAE Loss Implementation (10 pts)\n",
    "The VAE loss is a sum of the following two terms:\n",
    "\n",
    "1. The reconstruction loss: pixel-wise binary cross entropy beween the flattened reconstructed image `recon_x` and the flattened original image `x`. So, for each pixel, the prediction is obtainined from the reconstructed image and the label is the corresponding pixel in the original image. This loss across all the pixels is aggregated by summing the loss from each pixel.\n",
    "2. KL Divergence Loss: Using a standard gaussian prior $p(z)$ and the encoder's distribution $q_{\\theta} (z|x_i)$, you need to compute $KL(q_{\\theta} (z|x_i)||p(z))$. Note that $p(z)$ and $q_{\\theta} (z|x_i)$ are both multi-variate gaussian distributions. Suppose $q_{\\theta} (z|x_i)$ and $p(z)$ have means ${\\boldsymbol {\\mu }}_{0}$ and ${\\boldsymbol {\\mu }}_{1}$, and covariace ${\\boldsymbol {\\Sigma }}_{0}$ and ${\\boldsymbol {\\Sigma }}_{1}$ respectively, their KL divergence can be computed as follows (https://en.m.wikipedia.org/wiki/Multivariate_normal_distribution)):\n",
    "   ${\\displaystyle D_{\\text{KL}}({\\mathcal {N}}_{0}\\parallel {\\mathcal {N}}_{1})={1 \\over 2}\\left\\{\\operatorname {tr} \\left({\\boldsymbol {\\Sigma }}_{1}^{-1}{\\boldsymbol {\\Sigma }}_{0}\\right)+\\left({\\boldsymbol {\\mu }}_{1}-{\\boldsymbol {\\mu }}_{0}\\right)^{\\rm {T}}{\\boldsymbol {\\Sigma }}_{1}^{-1}({\\boldsymbol {\\mu }}_{1}-{\\boldsymbol {\\mu }}_{0})-k+\\ln {|{\\boldsymbol {\\Sigma }}_{1}| \\over |{\\boldsymbol {\\Sigma }}_{0}|}\\right\\}}$\n",
    "\n",
    "When computing the KL divergence loss, the constant $k$ can be ignored because it is just a constant. **Simplify the equation above and write the KL Divergence Loss as a function of the provided `mu` and `log_var`.**\n",
    "\n",
    "Hint 1: $p(z)$ is a standard gaussian.\n",
    "\n",
    "Hint 2: The covariance matrix ${\\boldsymbol {\\Sigma }}_{1}$ is a diagonal matrix whose entries can be found by using `log_var`.\n",
    "\n",
    "Hint 3: You don't need to do any 2d matrix multiplications after simplifying.\n",
    "\n",
    "**Note: Make sure there are no loops and no 2d matrix multiplications in your code for computing the loss. For both loss terms, compute the loss over the entire batch by summing the loss from each sample.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement `loss_function` in `submission.py`** then run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ydwer-BOeETv"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "from submission import loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4kauU8DfP4k"
   },
   "outputs": [],
   "source": [
    "best = 1000000\n",
    "# train and test code\n",
    "def train(model, epochs):\n",
    "    for epoch in range(1, epochs):\n",
    "      model.train()\n",
    "      train_loss = 0\n",
    "      for batch_idx, (data, _) in enumerate(train_loader):\n",
    "          data = data.to(device)\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          recon_batch, mu, log_var = model(data)\n",
    "          loss = loss_function(recon_batch, data.view(-1, 784), mu, log_var)\n",
    "\n",
    "          loss.backward()\n",
    "          train_loss += loss.item()\n",
    "          optimizer.step()\n",
    "\n",
    "          if batch_idx % 100 == 0:\n",
    "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                  epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                  100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "        \n",
    "      print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "      test(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon, mu, log_var = model(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data.view(-1, 784), mu, log_var).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    global best\n",
    "    if test_loss < best:\n",
    "        best = test_loss\n",
    "        torch.save(model.state_dict(), os.path.join(base_dir, f'best.pth'))\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVNiAB_9f9HF"
   },
   "outputs": [],
   "source": [
    "# Train the model for 10 epochs\n",
    "vae = train(vae, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoghqie-nzeM"
   },
   "source": [
    "### Part 1.3: Sample images from VAE (20 pts)\n",
    "Sample images from the VAE. To sample an image, you can sample vectors from a standard gaussian and pass it through the decoder. The `torch.randn()` function will be helpful for this part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement `sample_images` in `submission.py`** then run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5bwaNV_jUIv"
   },
   "outputs": [],
   "source": [
    "from submission import sample_images\n",
    "\n",
    "samples = sample_images(vae, num_samples=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOt-CPdNtYfg"
   },
   "outputs": [],
   "source": [
    "# visualize the generated samples\n",
    "def plot_images(images, rows=8, cols=8, figsize=(10, 10)):\n",
    "    images = images.view(64, 28, 28)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].cpu().numpy())\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzKr4qIXqWxw"
   },
   "source": [
    "### Q: Do you see all 10 digits? Do you notice that some digits are better quality than others? How would you try to improve the quality of the digits?\n",
    "\n",
    "> Indented block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** complete `get_embeddings()` function below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R00bUSC5Uihz"
   },
   "outputs": [],
   "source": [
    "# get embeddings for the first 100000 samples\n",
    "def get_embeddings():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    embeddings = []\n",
    "    embedding_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            ## TODO: obtain the 32-dimensional latent embedding vector for each\n",
    "            #        sample in the batch and append the batch_size x 32\n",
    "            #        dimensional matrix to the embeddings list\n",
    "            mu, log_var = vae.encoder(data)\n",
    "            embeddings.append(vae.sample(mu, log_var))\n",
    "\n",
    "            #################\n",
    "            embedding_labels.append(labels)\n",
    "            if i==99:\n",
    "              break\n",
    "    embeddings = torch.concatenate(embeddings)\n",
    "    embedding_labels = torch.concatenate(embedding_labels)\n",
    "    return embeddings, embedding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_h1UIS-A0u2-"
   },
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the obtained embeddigns to 2 and plot it on a\n",
    "# T-SNE Plot\n",
    "embeddings, embedding_labels = get_embeddings()\n",
    "embeddings = embeddings.cpu()\n",
    "embedding_labels = embedding_labels.cpu()\n",
    "\n",
    "print(\"Running TSNE to reduce dimensionality\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label in range(10):\n",
    "    plt.scatter(embeddings_2d[embedding_labels == label, 0],\n",
    "                embeddings_2d[embedding_labels == label, 1],\n",
    "                label=str(label), alpha=0.5)\n",
    "plt.title('t-SNE plot of MNIST embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXuQcZe0sL3Y"
   },
   "source": [
    "### Q: Do you notice that clusters in the center are smaller than the peripheral clusters on average? If so, why do you think is the case?\n",
    "\n",
    "> Indented block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8de32uHHXHCO"
   },
   "source": [
    "# Part 2: Diffusion Models (50 pts)\n",
    "\n",
    "In this section, you will develop a diffusion model to generate samples from a 2D synthetic dataset. Specifically, we will be using a dataset which consists of three rings and is visualized below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvTs9Qernqk0"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1pU2sne-Fvw82dYp2IwGw4dsFlTKzduzx\"\n",
    "width=\"400\"\n",
    "height=\"400\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wO3XF3p1jeh"
   },
   "source": [
    "We provide some helper functions below that will be used throughout the assignment. We also define a namedtuple, `ModelPrediction` that will store the prediction of our diffusion network at any given timestep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcFD7c3wW6UC"
   },
   "outputs": [],
   "source": [
    "def right_pad_dims_to(x, t):\n",
    "    padding_dims = x.ndim - t.ndim\n",
    "    if padding_dims <= 0:\n",
    "        return t\n",
    "    return t.view(*t.shape, *((1,) * padding_dims))\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_vqr8KP2Ck-"
   },
   "source": [
    "## Diffusion Background\n",
    "\n",
    "Diffusion models are latent variable models with latents $\\mathbf{z}  = \\{\\mathbf{z}_t | t\\in [0,1] \\}$ given by a forward diffusion process $q(\\mathbf{z}|\\mathbf{x})$, which defines a gradual transition from the data distribution, $\\mathbf{x} \\sim p(\\mathbf{x})$, to a Gaussian distribution. The Markovian forward process iteratively adds Gaussian noise to the data over time and satisfies\n",
    "\\begin{align*}\n",
    "q(\\mathbf{z}*t|\\mathbf{z}\\_s)=\\mathcal{N}(\\mathbf{z}\\_t; \\alpha*{t|s}\\mathbf{z}*s, (1-\\alpha*{t|s}^2)\\mathbf{I}),\\\\\n",
    "q(\\mathbf{z}\\_t|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z}\\_t; \\alpha_t\\mathbf{x}, (1-\\alpha_t^2)\\mathbf{I})\n",
    "\\end{align*}\n",
    "where $\\alpha_{t|s} = \\alpha_t/\\alpha_s$\n",
    "and $0 \\leq s < t \\leq 1$. Using the reparameterization trick, we can also write\n",
    "\\begin{align*}\n",
    "\\mathbf{z}\\_t = \\alpha_t\\mathbf{x} + \\sqrt{(1-\\alpha_t^2)}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n",
    "\\end{align*}\n",
    "\n",
    "The noise schedule, determined by $\\alpha_t\\in [0,1]$, monotonically decreases the signal-to-noise ratio (SNR), $\\lambda_t =\\frac{\\alpha_t^2}{1-\\alpha_t^2}$ as a function of the time, $t$, such that the initial latent is close to the original data, $\\mathbf{z}_0 \\approx \\mathbf{x}$, and the final latent becomes approximately Gaussian, $q(\\mathbf{z}_1) \\approx \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The forward process therefore defines a transition from the data distribution to a Gaussian distribution.\n",
    "\n",
    "Diffusion models define a generative process to invert the forward process. This specifies a transition from Gaussian noise, which can be sampled analytically, to the unknown data distribution. Inverting this process can be reduced to learning a denoising network $$\\hat{\\mathbf{x}}_\\theta(\\mathbf{z}_t, t) \\approx \\mathbf{x}$$ that reconstructs the clean data given some noisy latent and the time.\n",
    "\n",
    "In practice, people have found that parameterizing the denoising network as a noise prediction network $$\\hat{\\mathbf{\\epsilon}}_\\theta(\\mathbf{z}_t, t) \\approx \\mathbf{\\epsilon}$$ improves performance. Instead of predicting the clean data, the noise prediction network predicts the added noise. We therefore train the network with the following regression objective\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}*{t,\\mathbf{x}, \\epsilon} [ \\lVert\\hat{\\mathbf{\\epsilon}}_{\\theta}(\\mathbf{z}\\_t, t) - \\mathbf{\\epsilon} \\rVert_2^2\n",
    "\\end{align_}\n",
    "\n",
    "This loss function is the weighted variational lower bound of the log likelihood of the data under the forward diffusion process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLx7u1yF05Th"
   },
   "source": [
    "## Part 2.1: Diffusion Helper Functions (10 pts)\n",
    "\n",
    "1. We will use the popular cosine noise schedule that sets $\\alpha_t = \\cos(.5 \\pi t)$. Implement the `cosine_schedule()` function to map the timestep $t\\in[0,1]$ to $\\alpha_t^2$.\n",
    "\n",
    "2. Predicting the noise added to the data also provides us with an estimate of the clean data $$\\hat{\\mathbf{x}}_\\theta(\\mathbf{z}_t, t)$$ which we will use for sampling. Given $$\\mathbf{z}_t, \\alpha_t^2, \\hat{\\mathbf{\\epsilon}}_\\theta(\\mathbf{z}_t, t)$$ there is a closed-form solution for the estimated clean data. This closed-form solution can be derived directly from an equation provided in the background section. Implement `predict_start_from_noise()` to compute and return this estimate. Note that you should clamp any denominators to a minimum value of `clamp_min` to avoid dividing by numbers very close to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement `cosine_schedule(), predict_start_from_noise()` in `submission.py`** then run the following to import them here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdT-RrWiW6UC"
   },
   "outputs": [],
   "source": [
    "from submission import cosine_schedule, predict_start_from_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIDnFvkw9WSB"
   },
   "source": [
    "## Part 2.2: Diffusion Training and Sampling (40 pts)\n",
    "\n",
    "We define a `GaussianDiffusion()` class in `submission.py` that handles training and sampling.\n",
    "\n",
    "### Part 2.2.1: Training (20 pts)\n",
    "\n",
    "We train the network with the following regression objective\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}*{t,\\mathbf{x}, \\epsilon} [ \\lVert\\hat{\\mathbf{\\epsilon}}_{\\theta}(\\mathbf{z}\\_t, t) - \\mathbf{\\epsilon} \\rVert_2^2.\n",
    "\\end{align_}\n",
    "For this part, you will finish implementing the `forward()` function. You need to:\n",
    "\n",
    "1. Compute the noisy latent $z_t$.\n",
    "2. Compute the diffusion model predictions with `self.diffusion_model_predictions()`. The function accepts the noisy latents and the timesteps.\n",
    "3. Implement the regression loss from the background section. The output of the network should be the noise $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ use the create the noisy latent $\\mathbf{z}_t$.\n",
    "\n",
    "**Note: Implement `forward()` in `submission.py`.**\n",
    "\n",
    "### Part 2.2.2: Sampling (20 pts)\n",
    "\n",
    "In this part, you will implement the DDIM sampler, which is used for the reverse process of the diffusion model. The DDIM sampler enables sampling from a noisy latent representation $z_t$ at a higher noise level (time step $t$) to a less noisy latent representation $z_s$ at a lower noise level (time step $s$, where $s < t$). This gradual denoising of latent representations is achieved through the following equation:\n",
    "\n",
    "$$ z*s = \\alpha_s \\hat{\\mathbf{x}}*\\theta(\\mathbf{z}_t, t) + \\sqrt{1 - \\alpha_s^2} \\hat{\\mathbf{\\epsilon}}_\\theta(\\mathbf{z}\\_t, t).$$\n",
    "\n",
    "Your task is to complete the `ddim_sample()` function in the `GaussianDiffusion` class to implement the DDIM sampler for the reverse process.\n",
    "\n",
    "**Note: Implement `ddim_sample()` in `submission.py`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha4AP5YUSEmy"
   },
   "source": [
    "### Q: What are the sources of stochasticity in the DDIM sampler? How do different samples from the same initial draw of noise relate to each other, if at all? Write 2-3 sentences below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import GaussianDiffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLjCb0q1QxbD"
   },
   "source": [
    "## Part 2.3: Diffusion Model Architecture (5 pts)\n",
    "\n",
    "For this part, we provide the diffusion architecture for you. This diffusion architecture accepts the noisy latent and the timestep and predicts the added noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLRjmLT7RXpW"
   },
   "source": [
    "### Q: Briefly describe the diffusion architecture being used for this problem? How is the timestep information being incorporated? Write 3-4 sentences below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB9wlOaiW6UD"
   },
   "outputs": [],
   "source": [
    "def zero_init_(m):\n",
    "    nn.init.zeros_(m.weight)\n",
    "    if exists(m.bias):\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim = -1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** 0.5\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.normalize(x, dim = -1) * self.scale * self.gamma\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        mult = 4,\n",
    "        time_cond_dim = None,\n",
    "        dropout = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = RMSNorm(dim)\n",
    "        inner_dim = dim * mult\n",
    "        dim_out = dim\n",
    "\n",
    "        self.time_cond = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim*2),\n",
    "            SwiGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "        if exists(time_cond_dim):\n",
    "            self.time_cond = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_cond_dim, dim * 3),\n",
    "                Rearrange('b d -> b d')\n",
    "            )\n",
    "\n",
    "            zero_init_(self.time_cond[-2])\n",
    "        else:\n",
    "            zero_init_(self.net[-1])\n",
    "\n",
    "\n",
    "    def forward(self, x, time = None):\n",
    "        x = self.norm(x)\n",
    "        if exists(self.time_cond):\n",
    "            assert exists(time)\n",
    "            scale, shift, gate = self.time_cond(time).chunk(3, dim = 1)\n",
    "            x = (x * (scale + 1)) + shift\n",
    "\n",
    "        x = self.net(x)\n",
    "\n",
    "        if exists(self.time_cond):\n",
    "            x = x*gate\n",
    "\n",
    "        return x\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_layers = 3,\n",
    "        *,\n",
    "        time_cond_dim = None,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not exists(time_cond_dim):\n",
    "            time_cond_dim = dim\n",
    "\n",
    "        self.init_proj = nn.Linear(2, dim)\n",
    "        sinusoidal_pos_emb = SinusoidalPosEmb(dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinusoidal_pos_emb,\n",
    "            nn.Linear(time_cond_dim, time_cond_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_cond_dim, time_cond_dim)\n",
    "        )\n",
    "        self.nets = nn.ModuleList([FeedForward(dim, time_cond_dim = time_cond_dim, dropout = dropout) for _ in range(n_layers)])\n",
    "        self.norm = RMSNorm(dim)\n",
    "        self.output_proj = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x, alpha2 = None):\n",
    "\n",
    "        time_cond = self.time_mlp(alpha2*1000)\n",
    "\n",
    "        x = self.init_proj(x)\n",
    "\n",
    "        for net in self.nets:\n",
    "            x = x + net(x, time = time_cond)\n",
    "\n",
    "        return self.output_proj(self.norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjSzvCnCIFT5"
   },
   "source": [
    "This block of code defines the synthetic 2d dataset that we will be using to train our diffusion model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpMeyTU4W6UD"
   },
   "outputs": [],
   "source": [
    "def normalize(points):\n",
    "    \"\"\"\n",
    "    Normalize so that the points have zero mean and unit variance.\n",
    "    :param points: (N, 2)\n",
    "    \"\"\"\n",
    "    mean = points.mean()\n",
    "    std = points.std()\n",
    "    return (points - mean) / std\n",
    "\n",
    "class BaseSampler(object):\n",
    "    def __init__(self, radii: np.array, centers: np.array, width: float):\n",
    "        \"\"\"\n",
    "        Base sampler for rings and squares.\n",
    "        :param radii: radius of the rings or squares\n",
    "        :param centers\n",
    "        :param width: the width of each ring / square\n",
    "        \"\"\"\n",
    "        self.num_objects = radii.shape[0]  # Number of rings or squares\n",
    "        self.radii = np.array(radii, np.double)\n",
    "        self.centers = np.array(centers, np.double)\n",
    "        self.width = width\n",
    "\n",
    "class RingSampler(BaseSampler):\n",
    "    \"\"\"\n",
    "    Data sampler class to generate synthetic 2D distribution data.\n",
    "    The implementation considers only circle-like distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def sample(self, N):\n",
    "        \"\"\"\n",
    "        Samples from the 2D distribution.\n",
    "        Returns a sample of the shape (N, 2).\n",
    "        :param N: number of data points\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Assigns points to one of the K rings\n",
    "        K = self.num_objects\n",
    "        assert N % K == 0\n",
    "        indices = np.arange(0, K)\n",
    "        indices = np.tile(indices, N // K + 1)[:N]\n",
    "        indices = np.sort(indices)  # (0,0,0,0,1,1,1,1...\n",
    "        assert indices.shape[0] == N\n",
    "        centers = self.centers[indices]\n",
    "\n",
    "        radii_eps = (np.random.rand(N) - .5) * self.width\n",
    "        radii = self.radii[indices] + radii_eps\n",
    "\n",
    "        # Randomly assigns points on the ring\n",
    "        theta = np.random.rand(K, N // K) * 2 * np.pi\n",
    "        theta = np.sort(theta, 1).reshape(-1)[::-1]\n",
    "        x, y = np.rollaxis(centers, 1)\n",
    "        px = radii * np.cos(theta) + x\n",
    "        py = radii * np.sin(theta) + y\n",
    "\n",
    "        points = np.stack([px, py], axis=1)\n",
    "        points = normalize(points)\n",
    "        return points, indices\n",
    "\n",
    "class OlympicRingSampler(RingSampler):\n",
    "    def __init__(self, radii: np.array = np.ones(5), width: float = 0.5):\n",
    "        # The ratio between radii and centers is fixed\n",
    "        num_objects = radii.shape[0]\n",
    "        centers = np.array([(-140, 0), (0, 0), (140, 0), (-55, -50), (55, -50)], np.float32) / float(50)\n",
    "        centers = centers[:num_objects]\n",
    "        super(OlympicRingSampler, self).__init__(radii, centers, width)\n",
    "\n",
    "\n",
    "def scatter(points, enable_color_interpolation=True):\n",
    "    \"\"\"Draws a scatter, fine plots of the given points.\"\"\"\n",
    "    xlim, ylim = 3, 3\n",
    "    N = points.shape[0]\n",
    "    px, py = np.rollaxis(points, 1)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.xlim(-xlim, xlim)\n",
    "    plt.ylim(-ylim, ylim)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    if enable_color_interpolation:\n",
    "        colors = np.arange(0, N)\n",
    "    else:\n",
    "        colors = np.array([[0.525776, 0.833491, 0.288127]])\n",
    "    plt.scatter(px, py, s=1.0, marker='o', c=colors, cmap='rainbow', linewidths=0, alpha=1.0)\n",
    "\n",
    "\n",
    "class Synthetic2DDataset(Dataset):\n",
    "    def __init__(self, n_samples,):\n",
    "        super().__init__()\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        radii_uniform = np.array([1, 1, 1])\n",
    "        points, indices = OlympicRingSampler(radii_uniform).sample(n_samples)\n",
    "        self.points = normalize(points).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return np.array(self.points[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGcECQVEReH1"
   },
   "source": [
    "This cell visualizes the synthetic dataset that we will be using to train the model. The colors are purely for visualization purposes and are not meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rh18LCfcW6UD"
   },
   "outputs": [],
   "source": [
    "N = 150000\n",
    "shape = 'parallel_rings'\n",
    "print(f\"Plotting {shape} images.\")\n",
    "dataset = Synthetic2DDataset(N)\n",
    "points = dataset.points\n",
    "print(points.mean(), points.std())\n",
    "scatter(points)\n",
    "\n",
    "# save image\n",
    "plt.savefig(f\"part2_1.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd4chhaKRoU_"
   },
   "source": [
    "## Part 2.4 Diffusion Model Training and Evaluation (5 pts)\n",
    "\n",
    "We provide the code to train the diffusion model and draw samples from it. We\n",
    "then visualize the samples generated by the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjQPsPDRW6UE"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_loop(model, loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1g3Df8kyW6UE"
   },
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "\n",
    "n_samples = 500001\n",
    "dataset = Synthetic2DDataset(n_samples)\n",
    "loader = DataLoader(dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4abY4v1MW6UE"
   },
   "outputs": [],
   "source": [
    "model = GaussianDiffusion(ScoreNet(256, n_layers=6))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "NUM_EPOCHS = 25\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=NUM_EPOCHS * len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sp3XqZfaW6UE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for idx in range(NUM_EPOCHS):\n",
    "    loss = train_loop(model, loader, optimizer, scheduler, device)\n",
    "    print(f'Epoch {idx}: loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wSYM1__W6UE"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sampled_points, intermediate_preds = model.sample(50000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0dyxwS_W6UE"
   },
   "outputs": [],
   "source": [
    "sampled_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_2ap6YsW6UE"
   },
   "outputs": [],
   "source": [
    "intermediate_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4CoR-fPW6UE"
   },
   "outputs": [],
   "source": [
    "def sort_by_angles(arr):\n",
    "    # Convert cartesian coordinates to polar coordinates\n",
    "    r = np.sqrt(arr[:, 0]**2 + arr[:, 1]**2)\n",
    "    theta = np.arctan2(arr[:, 1], arr[:, 0])\n",
    "\n",
    "    # Combine angles and original data points\n",
    "    polar_arr = np.column_stack((theta, arr))\n",
    "\n",
    "    # Sort the array based on angles\n",
    "    sorted_indices = polar_arr[:, 0].argsort()\n",
    "\n",
    "    return sorted_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAaUyKjIW6UF"
   },
   "outputs": [],
   "source": [
    "sort_idx = sort_by_angles(intermediate_preds[:,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A03H0ibHSe0G"
   },
   "source": [
    "## Diffusion Sample Visualization\n",
    "\n",
    "We visualize the samples drawn from the diffusion model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAwHPyE4W6UF"
   },
   "outputs": [],
   "source": [
    "scatter(sampled_points[sort_idx].to('cpu').numpy())\n",
    "plt.savefig(f\"part2_2.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WhOzVUhSoLl"
   },
   "source": [
    "## Diffusion Process Visualization\n",
    "\n",
    "We visualize the evolution of the samples throughout the diffusion process. Points of the same color correspond to the same latent across time throughout the diffusion process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOno342mW6UF"
   },
   "outputs": [],
   "source": [
    "for idx in range(intermediate_preds.shape[1]):\n",
    "    scatter(intermediate_preds[:,idx,:][sort_idx])\n",
    "    if idx == intermediate_preds.shape[1]-1:\n",
    "        plt.savefig(f\"part2_3.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPWy9oeBSxzF"
   },
   "source": [
    "### Q: What do you observe about the diffusion sampling process? Does there appear to be any relationship between the initial Gaussian latent variable $\\mathbf{z}_1$ and the final sample from the data distribution $\\mathbf{z}_0$. Write 3-4 sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pxrb3Jsuv61y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
